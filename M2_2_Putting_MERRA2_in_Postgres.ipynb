{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code module picks up where the 00 module left off.  Data has been downloaded and the raw_data.csv file has been renamed to raw_data_lhbvlh_2014_2015.csv.\n",
    "\n",
    "We can check by looking at the directory Output/raw_data subdirectory. If you ran the code in 00 module, the file will be named raw_data.csv. the file posted to gitub and included in the example utility in the container is named raw_data_lhbvlh_2014_2015.csv.  First we'll load required packages and then check the raw_data directory to make sure the file we want is there.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pandas.io.sql as pdsqlio\n",
    "import psycopg2\n",
    "import getpass\n",
    "import re\n",
    "\n",
    "try:\n",
    "    os.chdir('Output/raw_data')\n",
    "except:\n",
    "    print('no such directory')\n",
    "print(os.getcwd())\n",
    "\n",
    "\n",
    "! ls -lta # bash shell directory command\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have data to import, so next we will connect to the database and do some setup work. We begin by building a connection to the postgres database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db=input('enter dbname: ')\n",
    "user=input('enter user: ')\n",
    "print('enter password: ')\n",
    "password=getpass.getpass()\n",
    "conn_str=\"host='localhost' dbname='\"+db+\"' user='\"+user+\"' password='\"+password+\"' options='-c search_path=sandbox,public'\"\n",
    "# the options allows us to add a schema called sandbox and be able to work in it\n",
    "conn=psycopg2.connect(conn_str)\n",
    "cur=conn.cursor()\n",
    "cur.execute('select version()')\n",
    "print(cur.fetchone()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are connected to the database.  For importing raw data that will be manipulated into a final format, lets put it into a sandbox schema to keep it separate from our 'good' data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_sql = \"SELECT exists(select schema_name FROM information_schema.schemata WHERE schema_name = 'sandbox');\"\n",
    "cur.execute(str_sql)\n",
    "conn.commit()\n",
    "schema_exists =cur.fetchone()[0]\n",
    "print(schema_exists)\n",
    "if schema_exists==False:\n",
    "    str_sql=\"create schema sandbox;\"\n",
    "    cur.execute(str_sql)\n",
    "    conn.commit()\n",
    "str_sql='select schema_name FROM information_schema.schemata;'\n",
    "cur.execute(str_sql)\n",
    "conn.commit()\n",
    "print(cur.fetchall())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created or have confirmed the existance of the sandbox schema, we should be able to use it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_sql = \"SELECT exists(select * FROM information_schema.tables WHERE table_schema = 'sandbox' and table_name = 'test');\"\n",
    "cur.execute(str_sql)\n",
    "conn.commit()\n",
    "table_exists =cur.fetchone()[0]\n",
    "print(table_exists)\n",
    "str_sql_drop = \"drop table sandbox.test;\"\n",
    "if table_exists==True:\n",
    "    cur.execute(str_sql_drop)\n",
    "\n",
    "str_sql = \"create table sandbox.test(name varchar(100), primary key(name));\"\n",
    "cur.execute(str_sql)\n",
    "str_sql = \"insert into sandbox.test(name) values('George');\"\n",
    "cur.execute(str_sql)\n",
    "str_sql = \"insert into sandbox.test(name) values('Jane');\"\n",
    "cur.execute(str_sql)\n",
    "str_sql = \"insert into sandbox.test(name) values('Judy');\"\n",
    "cur.execute(str_sql)\n",
    "str_sql = \"insert into sandbox.test(name) values('Elroy');\"\n",
    "cur.execute(str_sql)\n",
    "str_sql = \"insert into sandbox.test(name) values('Astro');\"\n",
    "cur.execute(str_sql)\n",
    "str_sql = \"insert into sandbox.test(name) values('Rosie');\"\n",
    "cur.execute(str_sql)\n",
    "conn.commit()\n",
    "\n",
    "str_sql = \"select * from sandbox.test;\"\n",
    "cur.execute(str_sql)\n",
    "conn.commit()\n",
    "print(cur.fetchall())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've tested our connection string and can add tables to the sandbox schema, we can detlete the test table then pull in the MERRA2 data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(str_sql_drop)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are cases where the file might be larger than the computer memory running this notebook. We will get just the first few rows to determine the file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname='raw_data_lhbvlh_2014_2015.csv'\n",
    "if os.path.isfile(fname):\n",
    "    data = pd.read_csv(fname, nrows=50)\n",
    "else:\n",
    "    raise SystemExit(\"Stop right there!\")\n",
    "    \n",
    "x=data.dtypes\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted in the 00 file, the MERRA2 data is fully described in https://gmao.gsfc.nasa.gov/pubs/docs/Bosilovich785.pdf. \n",
    "\n",
    "We can use the dtypes to create a table in our postgres database. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note this is strictly a drop and replace, it will not append data as written\n",
    "def maketable(fname):\n",
    "    type_map={'float64':'real',\n",
    "          'int64':'bigint',\n",
    "          'object':'timestamp'}\n",
    "    newtablename=re.sub('.csv','',fname,flags=re.I)\n",
    "    cur.execute(\"SELECT EXISTS(SELECT * FROM information_schema.tables \"+\n",
    "                \"WHERE table_schema = 'sandbox' AND table_name = '\"+newtablename+\"');\")\n",
    "    if cur.fetchone()[0]:\n",
    "        cur.execute(\"DROP TABLE \"+newtablename+';')\n",
    "        conn.commit()\n",
    "        msg='deleted and replaced'\n",
    "    else:\n",
    "        msg='created new'\n",
    "    data = pd.read_csv(fname,nrows=50)\n",
    "    types=data.dtypes\n",
    "    str_sql='CREATE TABLE '+newtablename+' ('\n",
    "    len_x=len(data.columns.tolist())\n",
    "    for x in data.columns.tolist():\n",
    "        str_sql=str_sql+x+' '+  type_map[str(types[x])]\n",
    "        if(x!=data.columns.tolist()[len_x-1]):\n",
    "            str_sql=str_sql + ', '\n",
    "        else :\n",
    "            str_sql=str_sql+ ');'\n",
    "    cur.execute(str_sql)\n",
    "    conn.commit()\n",
    "    return [msg,str_sql]  \n",
    "\n",
    "maketable(fname)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table has been created based on the structure of the data, so no we can import the csv directly into the database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tablename=re.sub('.csv','',fname,flags=re.I)\n",
    "f=open(fname)\n",
    "cur.copy_expert(\"copy sandbox.\"+tablename+\" from STDOUT delimiter ',' csv header\",f)\n",
    "conn.commit()\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is in the database, we can pull what we want into pandas, or continue working with data in teh database and doing data wrangling and munging directly with sql.\n",
    "\n",
    "Here's an example of pullin data into pandas using its ability to read sql.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_sql = \"select * from sandbox.\"+tablename+\" limit 5;\"\n",
    "data=pdsqlio.read_sql_query(str_sql,conn)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
