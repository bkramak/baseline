{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MERRA-2, from NASA, is an atmostpheric reanalysis that begins in 1980, but for wind plant energy analysis is usually only considered from a historic perspective from 1997-2000 due to significant changes in input data in the mid 1990's. See https://gmao.gsfc.nasa.gov/pubs/docs/Bosilovich785.pdf for more detail. The MERRA-2 data is comprised of a family of datasets, of which M2T1NXSLV hourly average and M2TMNXSLV monthly average are of the most interest for building wind plant power models. See pdf page 46, doc page 52 in same reference for a list of available variables included in the reanalysis.\n",
    "\n",
    "Data can be accessed via web interface using the simple subsetting wizard (ssw) or https://disc.gsfc.nasa.gov/SSW/ or programmatically via an API https://disc.gsfc.nasa.gov/information/howto?title=How%20to%20Use%20the%20Web%20Services%20API%20for%20Subsetting%20MERRA-2%20Data or directly with wget and prepared scripts https://disc.gsfc.nasa.gov/information/howto?title=How%20to%20Download%20Hourly%20MERRA-2%20Time%20Series%20at%20a%20Single%20Point.\n",
    "\n",
    "The notebook focuses on the programmatic approach, using python to pull data using the API. The main input is the latitude and longitude of the wind plant, in decimal degrees, with latitude <0 south of the equator, and longitude <0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by importing the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if any of the packages below are not installed, uncomment\n",
    "# line below and change packagename to package to be installed\n",
    "# assumes you have anaconda installed\n",
    "\n",
    "# !conda install --yes --prefix {sys.prefix} packagename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import getpass\n",
    "import json\n",
    "import urllib3\n",
    "import certifi\n",
    "import requests\n",
    "from time import sleep\n",
    "from http.cookiejar import CookieJar\n",
    "import urllib.request\n",
    "from urllib.parse import urlencode\n",
    "import bisect\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import ipywidgets as widgets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this code to work, you need a login at the NASA site.  Get a login here: https://urs.earthdata.nasa.gov/oauth/authorize?response_type=code&redirect_uri=https%3A%2F%2Fdisc.gsfc.nasa.gov%2Flogin%2Fcallback&client_id=C_kKX7TXHiCUqzt352ZwTQ \n",
    "\n",
    "After you have created an account, you must authorize that account for downloading data. See https://disc.gsfc.nasa.gov/earthdata-login. The update/approval is instantaneous, but download code below will not work until you do this step.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# force a pause\n",
    "input('did you authorize account?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download files to work properly, you must have a $HOME/.netrc file \n",
    "that contains the following text (configured with your own Earthdata userid and password): \n",
    "    machine urs.earthdata.nasa.gov login [userid] password [password]\n",
    "\n",
    "We will check to see if you have this file yet, and if not, prompt you for your username and password created at the link above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm=os.path.expanduser('~')\n",
    "fpathname=hm+\"/.netrc\"\n",
    "\n",
    "def createfile(fpathname) :\n",
    "    username = input(\"Enter username: \")\n",
    "    print('Enter password:')\n",
    "    password = getpass.getpass()\n",
    "    filetxt=\"machine urs.earthdata.nasa.gov login \"+username+\" password \"+password\n",
    "    f=open(fpathname,\"a+\")\n",
    "    f.write(filetxt)\n",
    "    f.close\n",
    "    print(\"created/added to .netrc\")\n",
    "    \n",
    "if os.path.exists(fpathname):\n",
    "    print(\"you have a .netrc file\")\n",
    "    f=open(fpathname)\n",
    "    for line in f:\n",
    "        #print(line)\n",
    "        if 'urs.earthdata.nasa.gov' in line:\n",
    "            print('it has urs.earthdata.nasa.gov')\n",
    "            break\n",
    "        else:\n",
    "            print(\"missing urs.earthdata.nasa.gov\")\n",
    "            createfile(fpathname)    \n",
    "else:\n",
    "    print(\".netrc does not exist, creating one...\")\n",
    "    createfile(fpathname) \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MERRA-2 data is for a matrix of datapoints spaced at 0.625 degrees intervals of longitude and 0.5% of latitude for a grid of 576 points longitude and 361 points latitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.arange(-180+0.625,180+0.625,0.625) # longitude -180,180 overlap so only one counted\n",
    "print('points of longitude: ' + str(len(x)))\n",
    "print(x[[0,1,2,573,574,575]])\n",
    "y=np.arange(-90,90.5,0.5) # latitude \n",
    "print('points of latitude: ' + str(len(y)))\n",
    "print(y[[0,1,2,358,359,360]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In almost all cases, the location we're interested in will not be located exactly at one of the MERRA-2 data grid points, so we build a bounding box around the point and get the data for the 4 surrounding points and interpolate between these 4 points to approximate site conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the location for La Haute Borne 4 turbine wind plant in France\n",
    "# This is the example in the NREL OpenOA notebooks, and KEI notebooks.\n",
    "input_lat = 48.451194\n",
    "input_lon = 5.589603\n",
    "\n",
    "def bounding_box(lat,lon):\n",
    "    #bisect only works with positive numbers so shift all positive\n",
    "    #then move back after indexes identified\n",
    "    x = np.linspace(-180.0,180.0,577)\n",
    "    y = np.linspace(-90.0,90.0,361)\n",
    "    #bisect doesn't work with negative numbers, so add 90/180 to make range positive\n",
    "    ind_y = bisect.bisect_left(y+90,lat+90)\n",
    "    ind_x = bisect.bisect_left(x+180,lon+180)\n",
    "\n",
    "    return [x[ind_x-1],y[ind_y-1],x[ind_x],y[ind_y]]\n",
    "\n",
    "bbox=bounding_box(input_lat,input_lon)\n",
    "print(bbox)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know where, we need to know when. This code only downloads 5 days of data, as it is time consuming. It is common to retrieve historic data back to Jan 1 1997 or Jan 1 2000. We also need data concurrent with the operational period of the wind plant to be studied, so we usually get data up through the present, usually 2 months behind the current date. As of 22 Oct 2020, data for MERRA-2 is available through the end of August 2020. There is usually a 4-6 week lag as the reanalysis data is prepared from observational data.\n",
    "\n",
    "Note that the data is stored daily, so it is also possible to subset specific time ranges within each day, as well as selecting specific date ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#begTime = '2010-01-01'\n",
    "begTime = '2014-01-01'\n",
    "#endTime = '2020-09-01'   # specifying 6 Jan gets data through 5 Jan.\n",
    "endTime = '2014-01-06'   # specifying 6 Jan gets data through 5 Jan.\n",
    "begHour = '00:30'\n",
    "endHour = '23:30'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we must specify the dataset to pull from, and within that dataset, specifically which variables in which we are interested. See the first link in the introductory paragraph, pdf page 46, doc page 52 for the list of available variables and descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product = 'M2T1NXSLV_5.12.4'\n",
    "varNames =['TS','T2MDEW','T2M', 'T10M', 'T850',\n",
    "           'H1000', 'H850','SLP', 'PS','PBLTOP',\n",
    "           'U2M','V2M','U10M','V10M','U50M','V50M','U850','V850']\n",
    "print(product)\n",
    "print(varNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to get the data.  We'll pass this off to a series of functions from the NASA site. The desired spatial and temporal constraints, along with the dataset and variable \n",
    "specifications, are stored in a JSON-based Web Service Protocol (WSP) structure, \n",
    "which is named “subset_request”. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_request = {\n",
    "    'methodname': 'subset',\n",
    "    'type': 'jsonwsp/request',\n",
    "    'version': '1.0',\n",
    "    'args': {\n",
    "        'role'  : 'subset',\n",
    "        'start' : begTime,\n",
    "        'end'   : endTime,\n",
    "        'box'   : bbox,\n",
    "        'crop'  : True, \n",
    "        'diurnalFrom': begHour, \n",
    "        'diurnalTo': endHour,\n",
    "        'diurnalMean': True,\n",
    "        'data': [{'datasetId': product,'variable' : varNames[0]},\n",
    "                 {'datasetId': product,'variable' : varNames[1]},\n",
    "                 {'datasetId': product,'variable' : varNames[2]},\n",
    "                 {'datasetId': product,'variable' : varNames[3]},\n",
    "                 {'datasetId': product,'variable' : varNames[4]},\n",
    "                 {'datasetId': product,'variable' : varNames[5]},\n",
    "                 {'datasetId': product,'variable' : varNames[6]},\n",
    "                 {'datasetId': product,'variable' : varNames[7]},\n",
    "                 {'datasetId': product,'variable' : varNames[8]},\n",
    "                 {'datasetId': product,'variable' : varNames[9]},\n",
    "                 {'datasetId': product,'variable' : varNames[10]},\n",
    "                 {'datasetId': product,'variable' : varNames[11]},\n",
    "                 {'datasetId': product,'variable' : varNames[12]},\n",
    "                 {'datasetId': product,'variable' : varNames[13]},\n",
    "                 {'datasetId': product,'variable' : varNames[14]},\n",
    "                 {'datasetId': product,'variable' : varNames[15]}]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the JSON-formatted subset_request is POSTed to the GES DISC server. The Job ID is extracted from the response and will be used later as a reference for the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a urllib PoolManager instance to make requests.\n",
    "http = urllib3.PoolManager(cert_reqs='CERT_REQUIRED',ca_certs=certifi.where())\n",
    "# Set the URL for the GES DISC subset service endpoint\n",
    "url = 'https://disc.gsfc.nasa.gov/service/subset/jsonwsp'\n",
    "\n",
    "def get_http_data(request,url):\n",
    "    hdrs = {'Content-Type': 'application/json',\n",
    "            'Accept'      : 'application/json'}\n",
    "    data = json.dumps(request)       \n",
    "    r = http.request('POST', url, body=data, headers=hdrs)\n",
    "    response = json.loads(r.data)   \n",
    "    # Check for errors\n",
    "    if response['type'] == 'jsonwsp/fault' :\n",
    "        print('API Error: faulty %s request' % response['methodname'])\n",
    "        sys.exit(1)\n",
    "    return response\n",
    "\n",
    "# Submit the subset request to the GES DISC Server\n",
    "response = get_http_data(subset_request,url)\n",
    "# Report the JobID and initial status\n",
    "myJobId = response['result']['jobId']\n",
    "print('Job ID: '+myJobId)\n",
    "print('Job status: '+response['result']['Status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the job is running on the GES DISC server. The next step is to construct another JSON WSP status_request, with methodname parameter set to 'GetStatus'. The args parameter contains the extracted Job ID. The status_request is submitted periodically to monitor the job status as it changes from 'Accepted' to 'Running' to '100% completed'. When the job is finished, check on the final status to ensure the job succeeded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct JSON WSP request for API method: GetStatus\n",
    "status_request = {\n",
    "    'methodname': 'GetStatus',\n",
    "    'version': '1.0',\n",
    "    'type': 'jsonwsp/request',\n",
    "    'args': {'jobId': myJobId}\n",
    "}\n",
    "\n",
    "# Check on the job status after a brief nap\n",
    "while response['result']['Status'] in ['Accepted', 'Running']:\n",
    "    sleep(5)\n",
    "    response = get_http_data(status_request,url)\n",
    "    status  = response['result']['Status']\n",
    "    percent = response['result']['PercentCompleted']\n",
    "    print ('Job status: %s (%d%c complete)' % (status,percent,'%'))\n",
    "if response['result']['Status'] == 'Succeeded' :\n",
    "    print ('Job Finished:  %s' % response['result']['message'])\n",
    "else : \n",
    "    print('Job Failed: %s' % response['fault']['code'])\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct JSON WSP request for API method: GetResult\n",
    "batchsize = 20\n",
    "results_request = {\n",
    "    'methodname': 'GetResult',\n",
    "    'version': '1.0',\n",
    "    'type': 'jsonwsp/request',\n",
    "    'args': {\n",
    "        'jobId': myJobId,\n",
    "        'count': batchsize,\n",
    "        'startIndex': 0\n",
    "    }\n",
    "}\n",
    "\n",
    "# Retrieve the results in JSON in multiple batches \n",
    "# Initialize variables, then submit the first GetResults request\n",
    "# Add the results from this batch to the list and increment the count\n",
    "results = []\n",
    "count = 0 \n",
    "response = get_http_data(results_request,url) \n",
    "count = count + response['result']['itemsPerPage']\n",
    "results.extend(response['result']['items']) \n",
    "\n",
    "# Increment the startIndex and keep asking for more results until we have them all\n",
    "total = response['result']['totalResults']\n",
    "while count < total :\n",
    "    results_request['args']['startIndex'] += batchsize \n",
    "    response = get_http_data(results_request,url) \n",
    "    count = count + response['result']['itemsPerPage']\n",
    "    results.extend(response['result']['items'])\n",
    "       \n",
    "# Check on the bookkeeping\n",
    "print('Retrieved %d out of %d expected items' % (len(results), total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the results into documents and URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "urls = []\n",
    "for item in results :\n",
    "    try:\n",
    "        if item['start'] and item['end'] : urls.append(item) \n",
    "    except:\n",
    "        docs.append(item)\n",
    "# Print out the documentation links, but do not download them\n",
    "print('\\nDocumentation:')\n",
    "for item in docs : print('\\n'+item['label']+': '+item['link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if getting a large date range comment out these lines of code with # sign\n",
    "print('\\n first 2 data URLS:')\n",
    "for item in urls[0:2] : print('\\n'+item['label']+': '+item['link'])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The final step is to invoke each HTTP_Services URL and download the data files. \n",
    "The contents of the label attribute are used here as the output file name, but \n",
    "the name can be any string. It is important to download each file one at a time, \n",
    "in series rather than in parallel, in order to avoid overloading the GES DISC servers.\n",
    "\n",
    "Download with Requests Library:\n",
    "For the request.get() module to work properly, you must have a $HOME/.netrc file \n",
    "that contains the following text (configured with your own Earthdata userid and password): \n",
    "    machine urs.earthdata.nasa.gov login [userid] password [password]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a login here: https://urs.earthdata.nasa.gov/oauth/authorize?response_type=code&redirect_uri=https%3A%2F%2Fdisc.gsfc.nasa.gov%2Flogin%2Fcallback&client_id=C_kKX7TXHiCUqzt352ZwTQ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the requests library to submit the HTTP_Services URLs anddownload the files. Note that they are being posted to a downloads folder within the current folder. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_folder='M2_JNB/downloads'\n",
    "try:\n",
    "    os.mkdir(download_folder)\n",
    "    print(\"Directory '% s' created\" % download_folder)\n",
    "except OSError as error:  \n",
    "    print(error) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one file per day.  It takes 10-20 seconds per file, so 5 files can take a minute or more to download. This translates roughly to 5-10 minutes per month, 1-2 hours per year, so a 20 year dataset can take more than 24 hours to download.\n",
    "\n",
    "As noted above when specifying end date, you have to go one past the date you want. the last item does not download for some reason. We drop the last item from urls to prevent getting errors when downloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a bandaid fix because code always broke on last URL, not sure why\n",
    "urls = urls[0:len(urls)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_files=len(urls)\n",
    "est_min=round((num_files*18)/60,1)\n",
    "est_hrs=round(est_min/60,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('This is the long part. you are downloading '+str(num_files)+' files.')\n",
    "print('This will take about '+str(est_min)+' minutes, or '+str(est_hrs)+' hours.' )\n",
    "print('The name of each file will be printed as it downloads.')\n",
    "response=input('Do you want to continue (y/n): ')\n",
    "if response in ['n','no','N','No','NO','nyet']:\n",
    "    raise SystemExit(\"Stop right there!\")\n",
    "\n",
    "out=widgets.Output()\n",
    "pb=widgets.IntProgress( value=0, min=0, max=num_files, \n",
    "                           step=1,description='Progress:', \n",
    "                           bar_style='success',orientation='horizontal')\n",
    "display(pb)\n",
    "print('\\nHTTP_services output:')\n",
    "\n",
    "for item in urls :\n",
    "\n",
    "    URL = item['link'] \n",
    "    result = requests.get(URL)\n",
    "    try:\n",
    "        result.raise_for_status()\n",
    "        outfn = download_folder+'/'+item['label']\n",
    "        f = open(outfn,'wb')\n",
    "        f.write(result.content)\n",
    "        f.close()\n",
    "        print(outfn)\n",
    "        pb.value +=1\n",
    "    except:\n",
    "        print('Error! Status code is %d for this URL:\\n%s' % (result.status_code,URL))\n",
    "        print('Help for downloading data is at https://disc.gsfc.nasa.gov/data-access')\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When downloading multiple years of files, it makes sense to check the files downloaded vs the target and also the file size. Files for the list of variables above are generally around 122 kB in size and we'll check that first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = os.listdir(download_folder)\n",
    "for f in file_list:\n",
    "    fpath=download_folder+\"/\"+f\n",
    "    print(f+' : ' + str(round(os.path.getsize(fpath)/1000))+' kB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll cycle through the original intended download list and make sure there is a file in {download_folder}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in urls :\n",
    "    path=download_folder+\"/\"+item['label']\n",
    "    print(item['label'] + \", downloaded =  \" + str(os.path.isfile(path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that download is complete and we've verified we got the files we wanted, it's time to mine the data from the files. The files are in netCDF format. There are several python packages for working with netCDF format, but xarray offers a simple conversion to pandas dataframe.\n",
    "\n",
    "We will read in and combine the data and save as csv to an Output/raw_data folder as the final step in this notebook. processing the data will be done in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainout_folder='M2_JNB'\n",
    "try:\n",
    "    os.mkdir(mainout_folder)\n",
    "    print(\"Directory '% s' created\" % mainout_folder)\n",
    "except OSError as error:  \n",
    "    print(error) \n",
    "\n",
    "rawout_folder='data'\n",
    "try:\n",
    "    os.mkdir(mainout_folder+'/'+rawout_folder)\n",
    "    print(\"Directory '% s' created\" % rawout_folder)\n",
    "except OSError as error:  \n",
    "    print(error) \n",
    "\n",
    "out_path=mainout_folder+'/'+rawout_folder\n",
    "out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data=[]\n",
    "for f in file_list:\n",
    "    fpath=download_folder+'/'+f\n",
    "    ds = xr.open_dataset(fpath)\n",
    "    df = ds.to_dataframe()\n",
    "    raw_data.append(df)\n",
    "print('\\nfiles imported into list: ',str(len(raw_data)) )\n",
    "raw_data = pd.concat(raw_data)\n",
    "\n",
    "print('\\nshape (rows and columns): ' + str(raw_data.shape))\n",
    "print('\\ncolumn names: ' + str(raw_data.columns))\n",
    "print('\\n\\n')\n",
    "print(raw_data.describe(include='all'))\n",
    "print('\\n\\n')\n",
    "print(raw_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we can write the combined raw data dataframe to csv format. Here we save 'raw_data.csv' out_path folder created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file='raw_data.csv'\n",
    "full_out_path=out_path+'/'+out_file\n",
    "raw_data.to_csv(full_out_path)\n",
    "\n",
    "print(os.listdir(out_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_in_raw=pd.read_csv(full_out_path)\n",
    "read_in_raw['time']=pd.to_datetime(read_in_raw['time'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "print(read_in_raw.dtypes)\n",
    "print('\\n\\n')\n",
    "print(read_in_raw.describe(include='all'))\n",
    "print('\\n\\n')\n",
    "print(read_in_raw)\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the specifications for the date range above, we set the start and end date begTime and endTime. we should make sure we got 4 datapoints for each - one for each the surrounding bounding box locations from our initial lat/lon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begTime=pd.to_datetime(begTime,format='%Y-%m-%d')\n",
    "begTime=begTime+timedelta(minutes=30)\n",
    "print(begTime)\n",
    "endTime=pd.to_datetime(endTime,format='%Y-%m-%d')\n",
    "endTime=endTime-timedelta(minutes=30) # need to drop an hour\n",
    "print(endTime)\n",
    "expected_timestamps= pd.date_range(start=begTime,end=endTime,freq='H')\n",
    "print(type(expected_timestamps))\n",
    "print(expected_timestamps)\n",
    "actual_timestamps=pd.to_datetime(read_in_raw['time'].unique(),format='%Y-%d-%mT%H:%M:%S').sort_values()\n",
    "print(type(actual_timestamps))\n",
    "print(actual_timestamps)\n",
    "\n",
    "print('\\n\\nDo they all match?  '+str(all(expected_timestamps==actual_timestamps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input('the end...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
